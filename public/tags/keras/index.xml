<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Keras on DNA confesses Data speak</title>
    <link>/tags/keras/</link>
    <description>Recent content in Keras on DNA confesses Data speak</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018 Ming &#39;Tommy&#39; Tang</copyright>
    <lastBuildDate>Sun, 24 Sep 2023 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/keras/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Generative AI: Text generation using Long short-term memory (LSTM) model</title>
      <link>/post/generative-ai-text-generation-using-long-short-term-memory-lstm-model/</link>
      <pubDate>Sun, 24 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/generative-ai-text-generation-using-long-short-term-memory-lstm-model/</guid>
      <description>In the world of deep learning, generating sequence data is a fundamental task. Typically, this involves training a network, often an RNN (Recurrent Neural Network) or a convnet (Convolutional Neural Network), to predict the next token or a sequence of tokens in a given sequence, using the preceding tokens as input. For example, when provided with the input “the cat is on the ma,” the network’s objective is to predict the next character, such as ‘t.</description>
    </item>
    
  </channel>
</rss>