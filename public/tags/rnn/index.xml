<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RNN on DNA confesses Data speak</title>
    <link>/tags/rnn/</link>
    <description>Recent content in RNN on DNA confesses Data speak</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018 Ming &#39;Tommy&#39; Tang</copyright>
    <lastBuildDate>Sun, 03 Sep 2023 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/rnn/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Long Short-term memory (LSTM) Recurrent Neural Network (RNN) to classify movie reviews </title>
      <link>/post/long-short-term-memory-lstm-recurrent-neural-network-rnn-to-classify-movie-reviews/</link>
      <pubDate>Sun, 03 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/long-short-term-memory-lstm-recurrent-neural-network-rnn-to-classify-movie-reviews/</guid>
      <description>A major characteristic of all neural networks I have used so far, such as densely connected networks and convnets (CNN) (see my previous post), is that they have no memory. Each input shown to them is processed independently, with no state kept in between inputs. In other words, they do not take into the context of the words (the words around the word).
Imagine you’re reading a book, and you want to understand the story by keeping track of what’s happening in the plot.</description>
    </item>
    
  </channel>
</rss>